# ğŸš€ Order-to-Delivery Analytics Pipeline

A **Data Engineering project** simulating an end-to-end data pipeline for analyzing **Order â†’ Delivery** performance using the **[Olist E-Commerce Dataset (Kaggle)](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)**.

The project aims to design a modern data architecture that includes a **Data Lake (Bronze/Silver layers)**, **dbt for transformation**, **Airflow for orchestration**, and **Metabase for analytics visualization**.

---

## ğŸ¯ Objectives

- Build a scalable **Data Lake** (Bronze â†’ Silver â†’ Data Warehouse)
- Standardize data transformation using **dbt**
- Automate and orchestrate ETL processes with **Apache Airflow**
- Visualize KPIs and business metrics using **Metabase / BI tools**

---

## ğŸ—ï¸ System Architecture

Kaggle CSV files
â†“
Data Lake (Bronze â†’ Silver)
â†“
Data Warehouse (PostgreSQL)
â†“
dbt models (staging, marts, KPIs)
â†“
Airflow DAGs (ETL orchestration)
â†“
Metabase Dashboard (BI visualization)


---

## ğŸ“‚ Repository Structure

ORDER_TO_DELIVERY_ANALYSTICS_PIPELINE/
â”‚
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/ # Airflow DAGs for orchestration
â”‚
â”œâ”€â”€ bi/ # BI dashboards (Metabase, Power BI)
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ landing/ # Raw CSV files from Kaggle
â”‚
â”œâ”€â”€ lake/
â”‚ â”œâ”€â”€ bronze/olist/ # Bronze layer (raw zone)
â”‚ â””â”€â”€ silver/olist/ # Silver layer (cleaned, normalized)
â”‚
â”œâ”€â”€ db/
â”‚ â””â”€â”€ ddl/ # SQL scripts for schema and table creation
â”‚
â”œâ”€â”€ dbt/ # dbt project (models, marts, tests)
â”‚
â”œâ”€â”€ docs/ # Documentation and diagrams
â”‚
â”œâ”€â”€ scripts/ # Custom Python ETL scripts
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ dev-requirements.txt # Dev/test dependencies
â”œâ”€â”€ .gitignore # Ignored local and system files
â””â”€â”€ README.md # Project documentation


---

## âš™ï¸ Setup & Installation

### ğŸ§© Requirements
- Python **3.10+**
- PostgreSQL **15+** (Database: `ecommerce_dw`)
- Virtual environment tool (`venv` or `virtualenv`)
- Apache Airflow **2.10+**

---

### ğŸš€ Setup Steps

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/dilambestall/order-to-delivery-analytics.git
cd order-to-delivery-analytics

# 2ï¸âƒ£ Create and activate virtual environment
python -m venv venv
venv\Scripts\activate      # Windows
source venv/bin/activate   # Linux/Mac

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
pip install -r dev-requirements.txt

# 4ï¸âƒ£ Create PostgreSQL database
createdb ecommerce_dw

ğŸŒ¬ï¸ Run Airflow Locally
# 1ï¸âƒ£ Set Airflow home (Linux/WSL)
export AIRFLOW_HOME=$(pwd)/airflow

# 2ï¸âƒ£ Initialize the Airflow database
airflow db init

# 3ï¸âƒ£ Create an admin user
airflow users create \
  --username admin \
  --password admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com

# 4ï¸âƒ£ Start Airflow webserver and scheduler
airflow webserver -p 8080
airflow scheduler

ğŸ“Š Dataset
Source: Olist Brazilian E-Commerce Public Dataset

After downloading, extract all CSV files into:
data/landing/

ğŸ§  Example DAGs

| DAG Name             | Description                                                             |
| -------------------- | ----------------------------------------------------------------------- |
| `hello_world_dag.py` | Simple test DAG to verify Airflow setup                                 |
| `etl_olist_daily.py` | ETL pipeline extracting Olist data and transforming into cleaned tables |

ğŸ‘¥ Contributors

@dilambestall

@hovngnvm

ğŸ§¾ License
This project is licensed under the MIT License.
See the LICENSE
 file for details.

 ğŸ’¡ Future Improvements
 Integrate CI/CD using GitHub Actions

Add dbt test coverage and documentation

Deploy Airflow with Docker Compose

Build interactive dashboards in Metabase
